{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Crash Course\n",
    "\n",
    "This notebook provides a comprehensive introduction to PySpark, covering everything from basic setup to advanced operations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [SparkSession and SparkContext](#2-sparksession-and-sparkcontext)\n",
    "3. [RDDs (Resilient Distributed Datasets)](#3-rdds-resilient-distributed-datasets)\n",
    "4. [DataFrames](#4-dataframes)\n",
    "5. [Data Loading and Saving](#5-data-loading-and-saving)\n",
    "6. [Data Transformations](#6-data-transformations)\n",
    "7. [Data Actions](#7-data-actions)\n",
    "8. [SQL Operations](#8-sql-operations)\n",
    "9. [Machine Learning with MLlib](#9-machine-learning-with-mllib)\n",
    "10. [Performance Optimization](#10-performance-optimization)\n",
    "11. [Best Practices](#11-best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "Java Home: /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\n",
      "Spark Home: /opt/homebrew/Cellar/apache-spark/4.0.0/libexec\n",
      "Python: /Users/vamsi_mbmax/Documents/Documents - raghuvamsiâ€™s MacBook Pro/VAM_Documents/01_vam_PROJECTS/LEARNING/proj_Productivity/dev_proj_Productivity/practise_prod_python_tools/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up environment variables\n",
    "os.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home'\n",
    "os.environ['SPARK_HOME'] = '/opt/homebrew/Cellar/apache-spark/4.0.0/libexec'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"Java Home: {os.environ.get('JAVA_HOME')}\")\n",
    "print(f\"Spark Home: {os.environ.get('SPARK_HOME')}\")\n",
    "print(f\"Python: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SparkSession and SparkContext\n",
    "\n",
    "SparkSession is the entry point for all Spark functionality. Let's create one and explore its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/21 17:59:38 WARN Utils: Your hostname, raghuvamsis-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 10.0.0.164 instead (on interface en0)\n",
      "25/06/21 17:59:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/21 17:59:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.0\n",
      "Spark Context: <SparkContext master=local[*] appName=PySpark Crash Course>\n",
      "Application Name: PySpark Crash Course\n",
      "Master: local[*]\n",
      "Default Parallelism: 10\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Crash Course\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Context: {sc}\")\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "RDDs are the fundamental data structure of Spark. Let's explore creating and manipulating RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "RDD partitions: 10\n",
      "RDD collect: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "Text RDD:\n",
      "Hello Spark\n",
      "PySpark is awesome\n",
      "Big Data processing\n",
      "Distributed computing\n"
     ]
    }
   ],
   "source": [
    "# Creating RDDs\n",
    "# 1. From a Python collection\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "\n",
    "print(\"Original numbers:\", numbers)\n",
    "print(\"RDD partitions:\", numbers_rdd.getNumPartitions())\n",
    "print(\"RDD collect:\", numbers_rdd.collect())\n",
    "\n",
    "# 2. From a text file (we'll create one first)\n",
    "sample_text = [\"Hello Spark\", \"PySpark is awesome\", \"Big Data processing\", \"Distributed computing\"]\n",
    "text_rdd = sc.parallelize(sample_text)\n",
    "\n",
    "print(\"\\nText RDD:\")\n",
    "for line in text_rdd.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RDD Transformations ===\n",
      "Squared numbers: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "Even numbers: [2, 4, 6, 8, 10]\n",
      "Words: ['Hello', 'Spark', 'PySpark', 'is', 'awesome', 'Big', 'Data', 'processing', 'Distributed', 'computing']\n",
      "Distinct numbers: [1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# RDD Transformations (Lazy operations)\n",
    "print(\"=== RDD Transformations ===\")\n",
    "\n",
    "# Map transformation\n",
    "squared_rdd = numbers_rdd.map(lambda x: x ** 2)\n",
    "print(\"Squared numbers:\", squared_rdd.collect())\n",
    "\n",
    "# Filter transformation\n",
    "even_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)\n",
    "print(\"Even numbers:\", even_rdd.collect())\n",
    "\n",
    "# FlatMap transformation\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "print(\"Words:\", words_rdd.collect())\n",
    "\n",
    "# Distinct transformation\n",
    "duplicate_numbers = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\n",
    "duplicate_rdd = sc.parallelize(duplicate_numbers)\n",
    "distinct_rdd = duplicate_rdd.distinct()\n",
    "print(\"Distinct numbers:\", distinct_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Actions (Trigger computation)\n",
    "print(\"=== RDD Actions ===\")\n",
    "\n",
    "# Count\n",
    "print(f\"Count of numbers: {numbers_rdd.count()}\")\n",
    "\n",
    "# First\n",
    "print(f\"First element: {numbers_rdd.first()}\")\n",
    "\n",
    "# Take\n",
    "print(f\"First 3 elements: {numbers_rdd.take(3)}\")\n",
    "\n",
    "# Reduce\n",
    "sum_result = numbers_rdd.reduce(lambda a, b: a + b)\n",
    "print(f\"Sum of all numbers: {sum_result}\")\n",
    "\n",
    "# Collect (use carefully with large datasets)\n",
    "all_numbers = numbers_rdd.collect()\n",
    "print(f\"All numbers: {all_numbers}\")\n",
    "\n",
    "# Sample\n",
    "sample_data = numbers_rdd.sample(False, 0.5, seed=42).collect()\n",
    "print(f\"Sample (50%): {sample_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DataFrames\n",
    "\n",
    "DataFrames provide a higher-level API with better optimization. Let's explore DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames\n",
    "print(\"=== Creating DataFrames ===\")\n",
    "\n",
    "# 1. From Python data\n",
    "data = [(\"Alice\", 25, \"Engineer\", 75000),\n",
    "        (\"Bob\", 30, \"Manager\", 85000),\n",
    "        (\"Charlie\", 35, \"Director\", 95000),\n",
    "        (\"Diana\", 28, \"Analyst\", 65000),\n",
    "        (\"Eve\", 32, \"Engineer\", 78000)]\n",
    "\n",
    "columns = [\"name\", \"age\", \"job\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nDataFrame Content:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. From Pandas DataFrame\n",
    "pandas_df = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C', 'D', 'E'],\n",
    "    'price': [10.5, 25.0, 15.75, 30.0, 12.25],\n",
    "    'quantity': [100, 50, 75, 25, 80],\n",
    "    'category': ['Electronics', 'Clothing', 'Electronics', 'Furniture', 'Clothing']\n",
    "})\n",
    "\n",
    "products_df = spark.createDataFrame(pandas_df)\n",
    "print(\"Products DataFrame:\")\n",
    "products_df.show()\n",
    "\n",
    "# 3. With explicit schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"score\", DoubleType(), True),\n",
    "    StructField(\"active\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "test_data = [(1, \"Test1\", 85.5, True),\n",
    "             (2, \"Test2\", 92.0, False),\n",
    "             (3, \"Test3\", 78.5, True)]\n",
    "\n",
    "test_df = spark.createDataFrame(test_data, schema)\n",
    "print(\"\\nTest DataFrame with explicit schema:\")\n",
    "test_df.printSchema()\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Basic Operations\n",
    "print(\"=== DataFrame Basic Operations ===\")\n",
    "\n",
    "# Select columns\n",
    "print(\"Select specific columns:\")\n",
    "df.select(\"name\", \"salary\").show()\n",
    "\n",
    "# Filter rows\n",
    "print(\"\\nFilter high earners (salary > 70000):\")\n",
    "df.filter(df.salary > 70000).show()\n",
    "\n",
    "# Add new column\n",
    "print(\"\\nAdd bonus column (10% of salary):\")\n",
    "df_with_bonus = df.withColumn(\"bonus\", df.salary * 0.1)\n",
    "df_with_bonus.show()\n",
    "\n",
    "# Rename column\n",
    "print(\"\\nRename 'job' to 'position':\")\n",
    "df.withColumnRenamed(\"job\", \"position\").show()\n",
    "\n",
    "# Drop column\n",
    "print(\"\\nDrop 'age' column:\")\n",
    "df.drop(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading and Saving\n",
    "\n",
    "Let's explore different ways to load and save data with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data files\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_dir = \"../data/pyspark_samples\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Save DataFrame as CSV\n",
    "print(\"Saving DataFrame as CSV...\")\n",
    "df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{data_dir}/employees.csv\")\n",
    "\n",
    "# Save DataFrame as JSON\n",
    "print(\"Saving DataFrame as JSON...\")\n",
    "df.coalesce(1).write.mode(\"overwrite\").json(f\"{data_dir}/employees.json\")\n",
    "\n",
    "# Save DataFrame as Parquet\n",
    "print(\"Saving DataFrame as Parquet...\")\n",
    "df.write.mode(\"overwrite\").parquet(f\"{data_dir}/employees.parquet\")\n",
    "\n",
    "print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from different formats\n",
    "print(\"=== Loading Data ===\")\n",
    "\n",
    "# Load CSV\n",
    "print(\"Loading from CSV:\")\n",
    "csv_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/employees.csv\")\n",
    "csv_df.show()\n",
    "csv_df.printSchema()\n",
    "\n",
    "# Load JSON\n",
    "print(\"\\nLoading from JSON:\")\n",
    "json_df = spark.read.json(f\"{data_dir}/employees.json\")\n",
    "json_df.show()\n",
    "\n",
    "# Load Parquet\n",
    "print(\"\\nLoading from Parquet:\")\n",
    "parquet_df = spark.read.parquet(f\"{data_dir}/employees.parquet\")\n",
    "parquet_df.show()\n",
    "parquet_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Transformations\n",
    "\n",
    "Let's explore various data transformation operations in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced DataFrame Transformations\n",
    "print(\"=== Advanced Transformations ===\")\n",
    "\n",
    "# Group by and aggregations\n",
    "print(\"Group by job and calculate average salary:\")\n",
    "df.groupBy(\"job\").agg(avg(\"salary\").alias(\"avg_salary\"), count(\"*\").alias(\"count\")).show()\n",
    "\n",
    "# Multiple aggregations\n",
    "print(\"\\nMultiple aggregations:\")\n",
    "df.groupBy(\"job\").agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    min(\"salary\").alias(\"min_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    count(\"*\").alias(\"count\")\n",
    ").show()\n",
    "\n",
    "# Window functions\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"\\nWindow functions - Rank by salary:\")\n",
    "window_spec = Window.orderBy(desc(\"salary\"))\n",
    "df.withColumn(\"rank\", row_number().over(window_spec)).show()\n",
    "\n",
    "# Partitioned window\n",
    "print(\"\\nPartitioned window - Rank within job category:\")\n",
    "window_spec_partitioned = Window.partitionBy(\"job\").orderBy(desc(\"salary\"))\n",
    "df.withColumn(\"job_rank\", row_number().over(window_spec_partitioned)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String operations\n",
    "print(\"=== String Operations ===\")\n",
    "\n",
    "# String functions\n",
    "df.select(\n",
    "    \"name\",\n",
    "    upper(\"name\").alias(\"name_upper\"),\n",
    "    lower(\"name\").alias(\"name_lower\"),\n",
    "    length(\"name\").alias(\"name_length\"),\n",
    "    substring(\"name\", 1, 3).alias(\"name_substr\")\n",
    ").show()\n",
    "\n",
    "# Conditional operations\n",
    "print(\"\\nConditional operations:\")\n",
    "df.select(\n",
    "    \"name\",\n",
    "    \"age\",\n",
    "    when(col(\"age\") < 30, \"Young\")\n",
    "    .when(col(\"age\") < 35, \"Middle\")\n",
    "    .otherwise(\"Senior\").alias(\"age_group\")\n",
    ").show()\n",
    "\n",
    "# Date operations (let's add some dates)\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add hire dates\n",
    "hire_dates = [(\"Alice\", datetime(2020, 1, 15)),\n",
    "              (\"Bob\", datetime(2019, 6, 10)),\n",
    "              (\"Charlie\", datetime(2018, 3, 22)),\n",
    "              (\"Diana\", datetime(2021, 9, 5)),\n",
    "              (\"Eve\", datetime(2020, 11, 30))]\n",
    "\n",
    "dates_df = spark.createDataFrame(hire_dates, [\"name\", \"hire_date\"])\n",
    "\n",
    "# Join with original DataFrame\n",
    "df_with_dates = df.join(dates_df, \"name\")\n",
    "\n",
    "print(\"\\nDate operations:\")\n",
    "df_with_dates.select(\n",
    "    \"name\",\n",
    "    \"hire_date\",\n",
    "    year(\"hire_date\").alias(\"hire_year\"),\n",
    "    month(\"hire_date\").alias(\"hire_month\"),\n",
    "    datediff(current_date(), \"hire_date\").alias(\"days_employed\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Actions and Aggregations\n",
    "\n",
    "Let's explore various actions and aggregation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Actions\n",
    "print(\"=== DataFrame Actions ===\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"DataFrame count:\", df.count())\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Describe - statistical summary\n",
    "print(\"\\nStatistical summary:\")\n",
    "df.describe().show()\n",
    "\n",
    "# Summary statistics for specific columns\n",
    "print(\"\\nSummary for salary:\")\n",
    "df.select(\"salary\").describe().show()\n",
    "\n",
    "# Collect data (be careful with large datasets)\n",
    "print(\"\\nCollected data:\")\n",
    "collected_data = df.collect()\n",
    "for row in collected_data[:3]:  # Show first 3 rows\n",
    "    print(row)\n",
    "\n",
    "# Convert to Pandas (for small datasets)\n",
    "print(\"\\nConvert to Pandas DataFrame:\")\n",
    "pandas_df = df.toPandas()\n",
    "print(type(pandas_df))\n",
    "print(pandas_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SQL Operations\n",
    "\n",
    "PySpark allows you to use SQL queries on DataFrames by creating temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Operations\n",
    "print(\"=== SQL Operations ===\")\n",
    "\n",
    "# Create temporary view\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "# Basic SQL queries\n",
    "print(\"Basic SELECT query:\")\n",
    "spark.sql(\"SELECT name, salary FROM employees WHERE salary > 70000\").show()\n",
    "\n",
    "print(\"\\nAggregation query:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT job,\n",
    "           AVG(salary) as avg_salary,\n",
    "           COUNT(*) as count\n",
    "    FROM employees\n",
    "    GROUP BY job\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nComplex query with window functions:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT name, job, salary,\n",
    "           RANK() OVER (ORDER BY salary DESC) as overall_rank,\n",
    "           RANK() OVER (PARTITION BY job ORDER BY salary DESC) as job_rank\n",
    "    FROM employees\n",
    "\"\"\").show()\n",
    "\n",
    "# Products analysis\n",
    "print(\"\\nProducts analysis:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT category,\n",
    "           COUNT(*) as product_count,\n",
    "           AVG(price) as avg_price,\n",
    "           SUM(quantity) as total_quantity\n",
    "    FROM products\n",
    "    GROUP BY category\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Machine Learning with MLlib\n",
    "\n",
    "Let's explore PySpark's machine learning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning with MLlib\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "print(\"=== Machine Learning Example ===\")\n",
    "\n",
    "# Create a larger dataset for ML\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "ml_data = []\n",
    "jobs = ['Engineer', 'Manager', 'Director', 'Analyst']\n",
    "for i in range(100):\n",
    "    job = random.choice(jobs)\n",
    "    age = random.randint(22, 60)\n",
    "    experience = random.randint(0, age - 22)\n",
    "\n",
    "    # Create salary based on job, age, and experience with some noise\n",
    "    base_salary = {'Engineer': 70000, 'Manager': 85000, 'Director': 100000, 'Analyst': 60000}[job]\n",
    "    salary = base_salary + (age * 1000) + (experience * 2000) + random.randint(-10000, 10000)\n",
    "\n",
    "    ml_data.append((f\"Person_{i}\", age, experience, job, salary))\n",
    "\n",
    "ml_df = spark.createDataFrame(ml_data, [\"name\", \"age\", \"experience\", \"job\", \"salary\"])\n",
    "\n",
    "print(\"ML Dataset:\")\n",
    "ml_df.show(10)\n",
    "ml_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ML\n",
    "print(\"=== Data Preparation for ML ===\")\n",
    "\n",
    "# String indexer for categorical variables\n",
    "job_indexer = StringIndexer(inputCol=\"job\", outputCol=\"job_index\")\n",
    "\n",
    "# Vector assembler to combine features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\", \"experience\", \"job_index\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create pipeline for data preparation\n",
    "prep_pipeline = Pipeline(stages=[job_indexer, assembler])\n",
    "prep_model = prep_pipeline.fit(ml_df)\n",
    "prepared_df = prep_model.transform(ml_df)\n",
    "\n",
    "print(\"Prepared data:\")\n",
    "prepared_df.select(\"features\", \"salary\").show(10, truncate=False)\n",
    "\n",
    "# Split data into training and testing\n",
    "train_df, test_df = prepared_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"\\nTraining set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "print(\"=== Linear Regression Model ===\")\n",
    "\n",
    "# Create and train linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"salary\")\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "print(\"Predictions vs Actual:\")\n",
    "predictions.select(\"name\", \"salary\", \"prediction\").show(10)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = RegressionEvaluator(labelCol=\"salary\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"\\nRoot Mean Square Error: {rmse:.2f}\")\n",
    "\n",
    "# Model coefficients\n",
    "print(f\"\\nModel Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Model Intercept: {lr_model.intercept:.2f}\")\n",
    "\n",
    "# Model summary\n",
    "print(f\"R-squared: {lr_model.summary.r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Optimization\n",
    "\n",
    "Let's explore various performance optimization techniques in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Optimization Techniques\n",
    "print(\"=== Performance Optimization ===\")\n",
    "\n",
    "# 1. Caching\n",
    "print(\"1. Caching DataFrames:\")\n",
    "# Cache frequently used DataFrames\n",
    "df.cache()\n",
    "print(f\"DataFrame is cached: {df.is_cached}\")\n",
    "\n",
    "# Persist with different storage levels\n",
    "from pyspark import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# 2. Partitioning\n",
    "print(\"\\n2. Partitioning:\")\n",
    "print(f\"Current partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition (expensive operation)\n",
    "repartitioned_df = df.repartition(4)\n",
    "print(f\"After repartitioning: {repartitioned_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce (cheaper than repartition for reducing partitions)\n",
    "coalesced_df = repartitioned_df.coalesce(2)\n",
    "print(f\"After coalescing: {coalesced_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 3. Broadcast variables (for small lookup tables)\n",
    "print(\"\\n3. Broadcast Variables:\")\n",
    "job_mapping = {\"Engineer\": \"ENG\", \"Manager\": \"MGR\", \"Director\": \"DIR\", \"Analyst\": \"ANA\"}\n",
    "broadcast_mapping = sc.broadcast(job_mapping)\n",
    "\n",
    "def map_job_code(job):\n",
    "    return broadcast_mapping.value.get(job, \"UNK\")\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "map_job_udf = udf(map_job_code, StringType())\n",
    "\n",
    "df.withColumn(\"job_code\", map_job_udf(\"job\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Query Optimization\n",
    "print(\"=== Query Optimization ===\")\n",
    "\n",
    "# Use explain() to see query plans\n",
    "print(\"Query execution plan:\")\n",
    "df.filter(df.salary > 70000).select(\"name\", \"salary\").explain()\n",
    "\n",
    "# 5. Join Optimization\n",
    "print(\"\\n5. Join Optimization:\")\n",
    "\n",
    "# Create a small lookup table\n",
    "job_info = [(\"Engineer\", \"Technical\", \"IC\"),\n",
    "            (\"Manager\", \"Leadership\", \"Management\"),\n",
    "            (\"Director\", \"Leadership\", \"Executive\"),\n",
    "            (\"Analyst\", \"Technical\", \"IC\")]\n",
    "\n",
    "job_df = spark.createDataFrame(job_info, [\"job\", \"department\", \"level\"])\n",
    "\n",
    "# Broadcast join (automatic for small tables)\n",
    "from pyspark.sql.functions import broadcast\n",
    "joined_df = df.join(broadcast(job_df), \"job\")\n",
    "print(\"Broadcast join result:\")\n",
    "joined_df.show()\n",
    "\n",
    "# 6. Column pruning and predicate pushdown\n",
    "print(\"\\n6. Optimized query with column pruning:\")\n",
    "# Only select needed columns and filter early\n",
    "optimized_query = df.filter(df.salary > 70000).select(\"name\", \"job\", \"salary\")\n",
    "optimized_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Best Practices and Tips\n",
    "\n",
    "Here are some important best practices when working with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices\n",
    "print(\"=== PySpark Best Practices ===\")\n",
    "\n",
    "print(\"\"\"\n",
    "1. **Data Formats:**\n",
    "   - Use Parquet for better performance and compression\n",
    "   - Avoid CSV for large datasets (slow parsing)\n",
    "   - Use Delta Lake for ACID transactions\n",
    "\n",
    "2. **Memory Management:**\n",
    "   - Cache/persist DataFrames that are used multiple times\n",
    "   - Use appropriate storage levels (MEMORY_ONLY, MEMORY_AND_DISK, etc.)\n",
    "   - Unpersist DataFrames when no longer needed\n",
    "\n",
    "3. **Partitioning:**\n",
    "   - Partition data by frequently filtered columns\n",
    "   - Avoid too many small partitions (< 128MB)\n",
    "   - Use coalesce() instead of repartition() when reducing partitions\n",
    "\n",
    "4. **Joins:**\n",
    "   - Use broadcast joins for small tables (< 10MB)\n",
    "   - Prefer bucketing for large table joins\n",
    "   - Filter data before joins when possible\n",
    "\n",
    "5. **UDFs (User Defined Functions):**\n",
    "   - Avoid UDFs when built-in functions are available\n",
    "   - Use vectorized UDFs (pandas UDFs) for better performance\n",
    "   - Consider using SQL expressions instead of UDFs\n",
    "\n",
    "6. **Resource Management:**\n",
    "   - Configure executor memory and cores appropriately\n",
    "   - Monitor Spark UI for performance bottlenecks\n",
    "   - Use dynamic allocation when possible\n",
    "\"\"\")\n",
    "\n",
    "# Example of efficient vs inefficient operations\n",
    "print(\"\\n=== Efficient vs Inefficient Examples ===\")\n",
    "\n",
    "# Inefficient: Multiple actions on same DataFrame\n",
    "print(\"Inefficient approach (multiple scans):\")\n",
    "high_earners = df.filter(df.salary > 70000)\n",
    "count1 = high_earners.count()\n",
    "avg_salary1 = high_earners.agg(avg(\"salary\")).collect()[0][0]\n",
    "print(f\"Count: {count1}, Avg Salary: {avg_salary1:.2f}\")\n",
    "\n",
    "# Efficient: Cache and reuse\n",
    "print(\"\\nEfficient approach (cache and reuse):\")\n",
    "high_earners_cached = df.filter(df.salary > 70000).cache()\n",
    "count2 = high_earners_cached.count()\n",
    "avg_salary2 = high_earners_cached.agg(avg(\"salary\")).collect()[0][0]\n",
    "print(f\"Count: {count2}, Avg Salary: {avg_salary2:.2f}\")\n",
    "\n",
    "# Clean up cache\n",
    "high_earners_cached.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring and Debugging\n",
    "print(\"=== Monitoring and Debugging ===\")\n",
    "\n",
    "# Spark UI information\n",
    "print(f\"Spark UI URL: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "# Configuration settings\n",
    "print(\"\\nImportant Spark configurations:\")\n",
    "conf = spark.sparkContext.getConf()\n",
    "print(f\"Executor Memory: {conf.get('spark.executor.memory', 'default')}\")\n",
    "print(f\"Executor Cores: {conf.get('spark.executor.cores', 'default')}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "\n",
    "# DataFrame lineage\n",
    "print(\"\\nDataFrame lineage (RDD lineage):\")\n",
    "complex_df = df.filter(df.salary > 70000).select(\"name\", \"salary\").orderBy(\"salary\")\n",
    "print(complex_df.rdd.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "print(\"=== Cleanup ===\")\n",
    "\n",
    "# Unpersist cached DataFrames\n",
    "df.unpersist()\n",
    "\n",
    "# Clear broadcast variables\n",
    "broadcast_mapping.unpersist()\n",
    "\n",
    "# Stop Spark session\n",
    "print(\"Stopping Spark session...\")\n",
    "spark.stop()\n",
    "print(\"Spark session stopped successfully!\")\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸŽ‰ Congratulations! You've completed the PySpark Crash Course!\n",
    "\n",
    "You've learned:\n",
    "âœ… Setting up PySpark environment\n",
    "âœ… Working with RDDs and DataFrames\n",
    "âœ… Data loading and saving in various formats\n",
    "âœ… Data transformations and actions\n",
    "âœ… SQL operations on DataFrames\n",
    "âœ… Machine learning with MLlib\n",
    "âœ… Performance optimization techniques\n",
    "âœ… Best practices and debugging tips\n",
    "\n",
    "Next steps:\n",
    "ðŸš€ Practice with real datasets\n",
    "ðŸš€ Explore Spark Streaming\n",
    "ðŸš€ Learn about Delta Lake\n",
    "ðŸš€ Deploy Spark applications on clusters\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
